{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a78351f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Uma breve introdução ao Machine Learning: Dia 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0db23d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Gabriel Wendell Celestino Rocha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed6737",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Material de um minicurso de introdução ao Machine Learning oferecido pelo [PET - Física](https://petfisica.home.blog)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6950e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "O conteúdo é mantido no [GitHub]() e distribuídos sob uma [licença BSD3](https://opensource.org/licenses/BSD-3-Clause)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc1dbf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Veja a tabela de conteúdos]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3541a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Este `Notebook` pode, opcionalmente, ser visto como uma [apresentação de slides](https://medium.com/learning-machine-learning/present-your-data-science-projects-with-jupyter-slides-75f20735eb0f). Clique [aqui]() para ver os slides online ou, para apresentar os slides localmente, use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6341400",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```Python\n",
    "$ jupyter nbconvert Dia2.ipynb --to slides --post serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ad858",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eee902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935f52ff",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "def opt_plot():\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.grid(True, linestyle=':', color='0.50')\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='both',which='minor', direction = \"in\",\n",
    "                        top = True,right = True, length = 5,width = 1,labelsize = 15)\n",
    "    plt.tick_params(axis='both',which='major', direction = \"in\",\n",
    "                        top = True,right = True, length = 8,width = 1,labelsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb8b88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Versões das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2f16e2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.9.8 64bit [MSC v.1929 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "8.0.1"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.19043"
        },
        {
         "module": "Matplotlib",
         "version": "3.5.1"
        },
        {
         "module": "Numpy",
         "version": "1.22.2"
        },
        {
         "module": "Pandas",
         "version": "1.4.0"
        },
        {
         "module": "Seaborn",
         "version": "0.11.2"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.9.8 64bit [MSC v.1929 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>8.0.1</td></tr><tr><td>OS</td><td>Windows 10 10.0.19043</td></tr><tr><td>Matplotlib</td><td>3.5.1</td></tr><tr><td>Numpy</td><td>1.22.2</td></tr><tr><td>Pandas</td><td>1.4.0</td></tr><tr><td>Seaborn</td><td>0.11.2</td></tr><tr><td colspan='2'>Sat Jun 04 17:35:15 2022 Hora Padrão de Buenos Aires</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.9.8 64bit [MSC v.1929 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 8.0.1 \\\\ \\hline\n",
       "OS & Windows 10 10.0.19043 \\\\ \\hline\n",
       "Matplotlib & 3.5.1 \\\\ \\hline\n",
       "Numpy & 1.22.2 \\\\ \\hline\n",
       "Pandas & 1.4.0 \\\\ \\hline\n",
       "Seaborn & 0.11.2 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sat Jun 04 17:35:15 2022 Hora Padrão de Buenos Aires} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.9.8 64bit [MSC v.1929 64 bit (AMD64)]\n",
       "IPython 8.0.1\n",
       "OS Windows 10 10.0.19043\n",
       "Matplotlib 3.5.1\n",
       "Numpy 1.22.2\n",
       "Pandas 1.4.0\n",
       "Seaborn 0.11.2\n",
       "Sat Jun 04 17:35:15 2022 Hora Padrão de Buenos Aires"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information Matplotlib, Numpy, Pandas, Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d614520",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Instalação:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5287ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```\n",
    "$ pip install version_information\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c9e6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775e917",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos o [módulo de decomposição](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) do `sklearn` abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9cdb4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "489ba0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baixando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8718f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455be28",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc640b33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionalidade dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561e539",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Chamamos o número de recursos (colunas) em um conjunto de dados de \"dimensionalidade\". Para aprender como os diferentes recursos estão relacionados, precisamos de amostras suficientes para obter uma imagem completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023d4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Por exemplo, imagine dividir cada recurso em seu valor mediano, então, no mínimo, gostaríamos de ter pelo menos uma amostra em cada um dos resultados $2^{D}$ bins ($D$ = dimensionalidade = $\\#$ de recursos = $\\#$ de colunas). Esta é uma barra muito baixa e requer apenas 8 amostras com $D=3$, mas requer $2^{30}>1$ bilhões de amostras com $D=30$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d3be3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Para ter uma ideia de quão bem amostrado é seu conjunto de dados, estime em quantos compartimentos você poderia dividir cada recurso (eixo) e termine com 1 amostra por compartimento (supondo que os recursos não sejam correlacionados). Um valor < 2 falha em nosso teste mínimo acima e qualquer coisa < 5 é uma potencial bandeira vermelha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a9222",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4149be66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No entanto, nem todos os recursos carregam informações iguais e a dimensionalidade efetiva de um conjunto de dados pode ser menor que o número de colunas. Como um exemplo extremo, considere os seguintes dados 2D que são efetivamente 1D, pois uma coluna tem um valor constante (zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88ca11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99f38dde",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**QUESTIONAMENTO:** Esses dados ainda são 1D se:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff5900",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. adicionamos uma pequena dispersão na 2ª dimensão?\n",
    "2. realizamos uma rotação de coordenadas para que $y\\approx mx$?\n",
    "3. $y\\approx f(x)$ onde $f(x)$ é não linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f013e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. A dispersão adiciona novas informações em uma segunda dimensão, mas podemos ignorá-las aproximadamente sob duas suposições:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804a247",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "> - A escala relativa das colunas `x` e `y` é significativa (o que quase certamente não é verdade se essas colunas tiverem dimensões diferentes - lembre-se de nossos comentários anteriores sobre normalização de dados).\n",
    "\n",
    "> - A origem da dispersão é devido a um erro de medição ou algum outro processo não informativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55783c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. A rotação não altera a dimensionalidade efetiva dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37d097",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Uma relação não linear entre `x` e `y` também não altera a dimensionalidade subjacente, pois poderíamos, em princípio, realizar uma mudança não linear de coordenadas para desfazê-la. No entanto, podemos esperar que as relações não lineares sejam mais difíceis de lidar do que as lineares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90400b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ca708",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83b8b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f6c0c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos `spectra_data` abaixo. Observe na tabela acima que parece ser severamente subamostrada com $N=200$, $D=500$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8816d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plote algumas linhas (amostras) de `spectra_data` usando `plt.plot(spectra_data.iloc[i], '.')` para ter uma ideia desse conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67157af9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb9bb0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**QUESTIONAMENTO:** Qual você acha que é a dimensionalidade efetiva desses dados? (*Dica*: quantos parâmetros independentes você precisaria para gerar esses dados?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ee711",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cada amostra é um gráfico de uma função suave com algum ruído adicionado. A função suave tem três componentes distintos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c824b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- dois picos, com localizações e formas fixas, e normalizações que variam independentemente.\n",
    "\n",
    "- um plano de fundo suave sem parâmetros livres. Como os dados podem ser reproduzidos apenas com parâmetros de normalização (exceto o ruído), eles têm uma dimensionalidade efetiva de $d=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff09ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe que a normalização relativa de cada recurso é significativa aqui, portanto, não gostaríamos de normalizar esses dados e perder essas informações. Referimo-nos a cada amostra como um \"espectro\", uma vez que se parece com espectros obtidos em diferentes áreas da física (astronomia, física nuclear, física de partículas, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b08ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Decomposição linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faa424",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "O objetivo de uma decomposição linear é identificar automaticamente combinações lineares das características originais que respondem pela maior parte da variação nos dados. Observe que estamos usando variância (spread) como proxy para \"informações úteis\", portanto, é essencial que a normalização relativa de nossos recursos seja significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649839df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Se representarmos nossos dados com $N\\times D$ a matriz $X$, então uma decomposição linear pode ser representada como a seguinte multiplicação de matrizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153eec9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![image](img/LinearDecomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f946d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A matriz $Y$ com $N\\times d$ é uma representação reduzida dos dados originais $X$, com $d<D$ novos recursos que são combinações lineares do recurso original $D$. Chamamos os novos recursos de \"variáveis latentes\", uma vez que eles já estavam presentes, mas apenas de forma implícita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bb20f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A matriz $M$ com $d\\times D$ especifica a relação entre os recursos antigos e novos: cada coluna é um vetor unitário para um novo recurso em termos dos recursos antigos. Observe que $M$ não é quadrado quando $d<D$ e vetores unitários geralmente não são mutuamente ortogonais (exceto para o método PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b47c44",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Uma decomposição linear não é exata (daí o $\\approxeq$ acima) e não há \"melhor\" prescrição para determinar $Y$ e $M$. Abaixo, revisamos as prescrições mais populares implementadas no módulo [`sklearn.decomposition`](https://scikit-learn.org/stable/modules/decomposition.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aac028",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "| Método | `sklearn` |\n",
    "| --- | --- |\n",
    "| [*Principal Component Analysis*](https://en.wikipedia.org/wiki/Principal_component_analysis) | [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) |\n",
    "| [*Factor Analysis*](https://en.wikipedia.org/wiki/Factor_analysis) | [FactorAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html) |\n",
    "| [*Non-negative Matrix Factorization*](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) | [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) |\n",
    "| [*Independent Component Analysis*](https://en.wikipedia.org/wiki/Independent_component_analysis) | [FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f34242",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Todos os métodos requerem que você especifique o número de variáveis latentes $d$ (mas você pode facilmente experimentar com valores diferentes) e são chamados usando (método = PCA, FactorAnalysis, NMF, FastICA):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf356c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```Python\n",
    "fit = decomposition.method(n_components=d).fit(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a58f22",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A decomposição resultante em $Y$ e $M$ é dada por:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6a198",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```Python\n",
    "M = fit.components_\n",
    "Y = fit.transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106c94d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "exceto para FastICA, onde `M = fit.mixing_.T`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a49cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quando $d<D$, nos referimos à decomposição como uma \"redução de dimensionalidade\". Uma visualização útil de quão efetivamente as variáveis latentes capturam as informações interessantes nos dados originais é calcular:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddafb98",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$X'=YM,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36aac3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "que reconstrói aproximadamente os dados originais e compara linhas (amostras) de $X'$ com o $X$ original. Eles não concordarão exatamente, mas se as diferenças parecerem desinteressantes (por exemplo, parecerem ruído), a redução de dimensionalidade foi bem-sucedida e você poderá usar em vez de para análise subsequente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d90dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos a função abaixo para demonstrar cada um deles (mas você pode ignorar seus detalhes, a menos que esteja interessado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6018b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def demo(method='PCA', d=2, data=spectra_data):\n",
    "    \n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    \n",
    "    if method == 'NMF':\n",
    "        # Todos os dados devem ser positivos.\n",
    "        assert np.all(X > 0)\n",
    "        # A análise inclui a média.\n",
    "        mu = np.zeros(D)\n",
    "    else:\n",
    "        mu = np.mean(X, axis=0)\n",
    "        \n",
    "    kwargs = dict(n_components=d)\n",
    "    if method == 'NMF':\n",
    "        kwargs['max_iter'] = 500\n",
    "    fit = eval('decomposition.' + method)(**kwargs).fit(X)\n",
    "    \n",
    "    # Verifique se a decomposição tem a forma esperada.\n",
    "    if method == 'FastICA':\n",
    "        M = fit.mixing_.T\n",
    "    else:\n",
    "        M = fit.components_\n",
    "    assert M.shape == (d, D)\n",
    "    Y = fit.transform(X)\n",
    "    assert Y.shape == (N, d)\n",
    "    \n",
    "    # Reconstrua X - mu do Y, M ajustado.\n",
    "    Xr = np.dot(Y, M) + mu\n",
    "    assert Xr.shape == X.shape\n",
    "    \n",
    "    # Pares de plotagem de variáveiss latentes.\n",
    "    columns = ['y{}'.format(i) for i in range(d)]\n",
    "    sns.pairplot(pd.DataFrame(Y, columns=columns))\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare algumas amostras de X e Xr.\n",
    "    fig = plt.figure(figsize=(8.5, 4))\n",
    "    for i,c in zip((0, 6, 7), sns.color_palette()):\n",
    "        plt.plot(X[i], '.', c=c, ms=3, alpha=0.5)\n",
    "        plt.plot(Xr[i], '-', c=c, lw=2)\n",
    "        \n",
    "    plt.xlim(-0.5, D+0.5)\n",
    "    plt.xlabel('Feature #')\n",
    "    plt.ylabel('Feature Value')\n",
    "    label = '{}(d={}): $\\sigma = {:.2f}$'.format(method, d, np.std(Xr - X))\n",
    "    plt.text(0.95, 0.9, label, horizontalalignment='right',\n",
    "             fontsize='x-large', transform=plt.gca().transAxes)\n",
    "    opt_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf559fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Análise do componente principal (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11507432",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "PCA é o método mais comumente usado para redução de dimensionalidade. A decomposição é especificada exclusivamente pela seguinte prescrição (mais detalhes [aqui](https://en.wikipedia.org/wiki/Principal_component_analysis#Computing_PCA_using_the_covariance_method)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc9acd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Ache os autovalores e autovetores de\n",
    "$$C=\\frac{1}{N-1}X^{T}X$$\n",
    "que é uma [estimativa empírica](https://en.wikipedia.org/wiki/Covariance#Calculating_the_sample_covariance) da matriz de covariância para os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8cb42",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Construa $M$ a partir dos autovetores, ordenados por autovalores decrescentes (que são todos positivos) e resolva as equações lineares resultantes para $Y$. Neste ponto a decomposição é exata com $d=D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097441f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Encolher $Y$ e $M$ de $D$ para $d$ linhas ($M$) ou colunas ($Y$), o que torna a decomposição aproximada enquanto descarta a menor quantidade de variação nos dados originais (que usamos como proxy para \"informações úteis\").\n",
    "![image](img\\PCAdecomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f2c2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A matriz completa $M$ (antes do encolhimento $D\\rightarrow d$) é ortogonal, ou seja $M^{T}=M^{-1}$ , e satisfaz $X^{T}X=M^{T}\\Lambda M$, onde $\\Lambda$ é uma matriz diagonal dos autovalores decrescentes. Observe que esta descrição ignora alguns detalhes que serão explorados na lista de problemas propostos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac5292",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As variáveis latentes resultantes são *estatisticamente não correlacionadas* (o que é uma afirmação mais fraca do que *estatisticamente independente* - veja abaixo), ou seja, os [coeficientes de correlação](https://en.wikipedia.org/wiki/Correlation_coefficient) entre as diferentes colunas de $Y$ são aproximadamente zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b2dfc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\rho(j,k)=\\frac{Y_{j}\\cdot Y_{k}}{|Y_{j}|\\cdot|Y_{k}|}\\approxeq0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089448a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A demonstração do PCA abaixo mostra um gráfico de pares das variáveis latentes de uma decomposição com $d=2$, seguido por uma reconstrução de algumas amostras (curvas vermelhas) comparadas com as originais (pontos vermelhos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d128739",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Observe que as amostras reconstruídas são, em certo sentido, melhores que as originais, pois o ruído original foi associado a um pequeno valor próprio que foi cortado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d42fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4442ec2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**QUESTIONAMENTO:** Quantos *clusters* você espera ver no gráfico de dispersão de `y0` versus `y1` acima com base no que você sabe sobre esse conjunto de dados? Você consegue identificar esses clusters no gráfico acima?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12081254",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esperamos ver 4 clusters, correspondendo a espectros com:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0d177",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Sem picos.\n",
    "\n",
    "- Apenas o pico inferior.\n",
    "\n",
    "- Apenas o pico superior.\n",
    "\n",
    "- Ambos os picos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7019ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Já vimos que esses dados podem ser gerados a partir de dois valores de fluxo, dando a normalização de cada pico. Vamos supor que `y0` e `y1` estejam relacionados a esses fluxos para identificar os clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cfc76",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Pontos próximos (-2000, -2000), com muito pouco spread.\n",
    "\n",
    "- Pontos ao longo da linha horizontal com `y0 ~ -2000`.\n",
    "\n",
    "- Pontos ao longo da linha diagonal.\n",
    "\n",
    "- Pontos espalhados entre as duas linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01ba6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Análise de fatores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb299a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A análise de fatores (FA) geralmente produz resultados semelhantes à PCA, mas é conceitualmente diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca09e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tanto o PCA quanto o FA assumem implicitamente que os dados são amostrados aproximadamente de uma Gaussiana multidimensional. O PCA então encontra os eixos principais do elipsóide multidimensional resultante, enquanto o FA é baseado em um modelo de como os dados originais são gerados a partir das variáveis latentes. Especificamente, o FA busca variáveis latentes que são unidades gaussianas não correlacionadas e permite diferentes níveis de ruído em cada característica, assumindo que esse ruído não está correlacionado com as variáveis latentes. O PCA não distingue entre \"sinal\" e \"ruído\" e assume implicitamente que os autovalores grandes são mais parecidos com sinal e os pequenos mais parecidos com ruído."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae17104",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quando as suposições de FA sobre os dados (de variáveis latentes gaussianas com ruído não correlacionado adicionado) estão corretas, certamente é a melhor escolha, em princípio. Na prática, a decomposição FA é mais cara e requer um algoritmo iterativo de Expectation-Maximization (EM). Normalmente, você deve tentar os dois, mas prefira o PCA mais simples quando os resultados forem indistinguíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5620488",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091c1ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Fatoração de matriz não negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a0f36",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A maioria das fatorações lineares começa centralizando cada recurso em torno de sua média sobre as amostras:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb860",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$X_{ij}\\rightarrow X_{ij}-\\mu\\text{ }\\text{ }\\text{ , }\\text{ }\\text{ }\\mu_{ij}\\equiv\\frac{1}{N}\\sum_{i}X_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc211b39",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Como resultado, as variáveis latentes são igualmente prováveis de serem positivas ou negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b30df7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A fatoração de matriz não negativa (NMF) assume que os dados são uma superposição linear (possivelmente ruidosa) de diferentes componentes, o que geralmente é uma boa descrição dos dados resultantes de um processo físico. Por exemplo, o espectro de uma galáxia é uma superposição dos espectros de suas estrelas constituintes, e o espectro de uma amostra radioativa é uma superposição dos decaimentos de seus isótopos instáveis constituintes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f64c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Esses processos só podem **adicionar** dados, portanto, os elementos de $Y$ e $M$ devem ser todos $\\geq0$ se as variáveis latentes descreverem misturas aditivas de diferentes processos. A fatoração NMF garante que ambos $Y$ e $M$ são positivos, e requer que a entrada $X$ também seja positiva. No entanto, não há garantia de que as variáveis latentes não negativas encontradas pelo NMF sejam devidas a misturas físicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b903fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Como o NMF não subtrai internamente as médias $\\mu_{ij}$, geralmente precisa de um componente adicional para modelar a média. Para `spectra_data` então, devemos usar `d = 3` para NMF para comparar com PCA usando `d = 2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc858543",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f010a33c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para ver a importância da variável extra latente, tente com `d = 2` e observe como as amostras são mal reconstruídas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883630b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411d3f82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Análise de componentes independentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec0b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A decomposição linear final que consideraremos é a ICA, onde o objetivo é encontrar variáveis latentes $Y$ que sejam *estatisticamente independentes*, o que é uma afirmação mais forte do que a garantia *estatisticamente não correlacionada* da PCA. Vamos formalizar a definição de independência em breve, mas a ideia básica é que a probabilidade conjunta de uma amostra ocorrer com variáveis latentes $y_{1},y_{2},y_{3},\\ldots$ pode ser fatorado em probabilidades independentes para cada componente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db64601",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$P(y_{1},y_{2},y_{3},\\ldots)=P(y_{1})P(y_{2})P(y_{3})\\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ac813",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O ICA tem algumas ambiguidades inerentes: tanto a ordenação quanto o dimensionamento das variáveis latentes são arbitrários, ao contrário do PCA. No entanto, na prática, as amostras reconstruídas com ICA muitas vezes parecem semelhantes às reconstruções de PCA e FA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbbf17",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "O ICA também é usado para [separação de sinal cego](https://en.wikipedia.org/wiki/Signal_separation), onde geralmente $d=N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044fc91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7e027c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Comparação de Métodos Lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3285b5d",
   "metadata": {},
   "source": [
    "Para comparar os quatro métodos acima, plote seus \"vetores unitários\" normalizados (linhas da matriz $M$). Observe que apenas as curvas NMF são sempre positivas, como esperado. No entanto, embora todos os métodos forneçam excelentes reconstruções dos dados originais, todos eles também misturam os dois picos. Em outras palavras, nenhum dos métodos descobriu as variáveis latentes naturais do processo físico subjacente: as normalizações de pico individuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8f600",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad0cb4a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. PCA ponderado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6a7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Os algoritmos lineares apresentados acima funcionam bem com dados ruidosos, mas não têm como tirar proveito de dados que incluem sua própria estimativa do nível de ruído. No caso mais geral, cada elemento da matriz $X$ de dados possui uma estimativa de erro RMS $\\delta X$ correspondente, com valores $\\rightarrow\\infty$ usados para indicar dados ausentes. Na prática, é conveniente substituir $\\delta X$ por uma matriz de pesos $W$ onde valores zero indicam dados ausentes. Para dados com erros gaussianos, $X_{ij}\\pm\\delta X_{ij}$, o peso apropriado é geralmente a *variância inversa* $I\\equiv\\delta X_{ij}^{-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033123a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Delchambre 2015](https://arxiv.org/abs/1412.4533) revisa várias abordagens para calcular componentes principais de dados ponderados e propõe um novo esquema onde, em vez de subtrair a média usual, subtraímos uma média ponderada,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44718d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\mu_{j}=\\frac{\\sum_{i=1}^{N}W_{ij}X_{ij}}{\\sum_{i=1}^{N}W_{ij}},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695c24b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "com $W=\\sqrt{I}$, e, em vez de decompor $C=X^{T}X$, nós decompomos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2ebf4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$C=\\frac{(WX)^{T}(WX)}{W^{T}W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f6998",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A classe abaixo implementa este esquema seguindo a mesma API da classe sklearn PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86798d45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use scipy.linalg.eigh ao invés de np.linalg.eigh uma vez que nos permite encontrar\n",
    "# os autovetores com os maiores autovalores.\n",
    "import scipy.linalg\n",
    "\n",
    "class WeightedPCA(object):\n",
    "    \"\"\"Implementa o esquema PCA ponderado do Delchambre 2015.\n",
    "    \n",
    "    Consulte https://arxiv.org/abs/1412.4533 and the more complete implementation\n",
    "    in https://github.com/jakevdp/wpca.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def prepare(self, X, ivar=None):\n",
    "        if ivar is None:\n",
    "            W = np.ones_like(X)\n",
    "        else:\n",
    "            assert np.all(ivar >= 0)\n",
    "            W = np.sqrt(ivar)\n",
    "        # Calcule a média ponderada dos dados usando a Eq (2).\n",
    "        self.mu = np.sum(W * X, axis=0) / np.sum(W, axis=0)\n",
    "        # Subtraia a média ponderada dos dados.\n",
    "        X = X - self.mu\n",
    "        # Aplique pesos aos dados (média subtraída).\n",
    "        X *= W\n",
    "        return X, W\n",
    "\n",
    "    def fit(self, X, ivar=None):\n",
    "        X, W = self.prepare(X, ivar)\n",
    "        # Calcule a covariância ponderada..\n",
    "        C = np.dot(X.T, X)\n",
    "        C /= np.dot(W.T, W)\n",
    "        # Encontre os autovetores e autovalores de C\n",
    "        _, D = X.shape\n",
    "        evals, evecs = scipy.linalg.eigh(C, eigvals=(D - self.n_components, D - 1))\n",
    "        # Salve os resultados.\n",
    "        self.components_ = evecs[:, ::-1].T\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, ivar=None):\n",
    "        X, W = self.prepare(X, ivar)\n",
    "        N, _ = X.shape\n",
    "        Y = np.zeros((N, self.n_components))\n",
    "        for i in range(N):\n",
    "            cW = self.components_ * W[i]\n",
    "            cWX = np.dot(cW, X[i])\n",
    "            cWc = np.dot(cW, cW.T)\n",
    "            Y[i] = np.linalg.solve(cWc, cWX)\n",
    "        return Y\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return self.mu + np.dot(X, self.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f4905",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para estudar esses esquemas, atribuiremos pesos a `spectra_data` assumindo que cada valor $X_{ij}$ é o resultado de um processo de Poisson, então tem variância inversa $I=X_{ij}^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0188",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A função abaixo nos permite, opcionalmente, adicionar ruído extra que varia entre os espectros e remover pedaços aleatórios de dados (definindo seus pesos como zero). Como de costume, ignore os detalhes desta função, a menos que esteja interessado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68401bd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_pca(d=2, add_noise=None, missing=None, data=spectra_data, seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    X = data.values.copy()\n",
    "    N, D = X.shape\n",
    "    # Calcule as variâncias inversas assumindo flutuações de Poisson em X.\n",
    "    ivar = X ** -1\n",
    "    \n",
    "    # Adicione algum ruído gaussiano com um RMS linearmente variável, se solicitado.\n",
    "    if add_noise:\n",
    "        start, stop = add_noise\n",
    "        assert start >= 0 and stop >= 0\n",
    "        extra_rms = np.linspace(start, stop, D)\n",
    "        ivar = (X + extra_rms ** 2) ** -1\n",
    "        X += gen.normal(scale=extra_rms, size=X.shape)\n",
    "        \n",
    "    # Remova alguma fração de dados de cada amostra, se solicitado.\n",
    "    if missing:\n",
    "        assert 0 < missing < 0.5\n",
    "        start = gen.uniform(high=(1 - missing) * D, size=N).astype(int)\n",
    "        stop = (start + missing * D).astype(int)\n",
    "        for i in range(N):\n",
    "            X[i, start[i]:stop[i]] = ivar[i, start[i]:stop[i]] = 0.\n",
    "            \n",
    "    # Execute o ajuste.\n",
    "    fit = WeightedPCA(n_components=d).fit(X, ivar)\n",
    "    Y = fit.transform(X, ivar)\n",
    "    Xr = fit.inverse_transform(Y)\n",
    "    \n",
    "    # Mostre a reconstrução de algumas amostras.\n",
    "    fig = plt.figure(figsize=(8.5, 4))\n",
    "    for i,c in zip((0, 6, 7), sns.color_palette()):\n",
    "        plt.plot(X[i], '.', c=c, ms=3, alpha=0.5)\n",
    "        plt.plot(Xr[i], '-', c=c, lw=2)\n",
    "    plt.xlim(-0.5, D+0.5)\n",
    "    plt.xlabel('Feature #')\n",
    "    plt.ylabel('Feature Value')\n",
    "    opt_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a68f2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primeiro, verifique se recuperamos resultados semelhantes ao PCA padrão sem ruído adicionado ou dados ausentes. Os resultados não são idênticos (mas presumivelmente melhores agora) porque estamos levando em conta o fato de que os erros relativos são maiores para fluxos mais baixos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea2b98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca1b514",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Em seguida, varie o nível de ruído no espectro. Os erros maiores dificultam a fixação dos componentes principais, levando a reconstruções mais ruidosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed6588",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2938870",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "512e4c72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finalmente, remova 10% dos dados de cada amostra (mas, crucialmente, 10% diferente de cada amostra). Observe como isso nos permite fazer uma estimativa sensata dos dados ausentes! (os estatísticos chamam isso de [imputação](https://en.wikipedia.org/wiki/Imputation_(statistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a47b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1fb9c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Redução de Dimensionalidade Não Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2a11f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Os métodos acima encontram variáveis latentes que são funções lineares das características originais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289f18e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Existem também métodos não lineares:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969fd5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Quando funcionam, os resultados são [espetaculares](https://scikit-learn.org/stable/modules/manifold.html) (veja também [aqui](https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html)).\n",
    "\n",
    "- No entanto, eles geralmente são muito [sensíveis à sua escolha de hiperparâmetros](https://github.com/scikit-learn/scikit-learn/issues/10530).\n",
    "\n",
    "- Eu recomendo que você sempre comece com um método linear e só prefira um modelo não linear se ele tiver um desempenho claramente melhor e fornecer resultados consistentes e robustos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fed41",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Na aula do último dia deste minicurso iremos dar um mergulho mais profundo em métodos não lineares. Uma ideia-chave é o \"*truque do kernel*\", que também é central para o poder das redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c9a12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 9. Compressão Orientada por Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965868f4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A maioria dos métodos de redução de dimensionalidade assume que a variância é uma boa proxy para \"informação\". No entanto, quando você tem um bom modelo generativo de seus dados, pode fazer muito melhor. Em particular, existe um algoritmo de compactação ideal para dados gerados por um modelo $n$ com parâmetros que reduzirão todo o seu conjunto de dados a $n$ números! Consulte este [artigo de 2018](https://doi.org/10.1093/mnras/sty819) para obter detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e9fed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](https://c.tenor.com/hEOM8E4epvgAAAAC/hahaha-thats-all-folks.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809cafd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
