{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f48da6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Uma breve introdução ao Machine Learning: Dia 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc2b90",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Gabriel Wendell Celestino Rocha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc515a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Material de um minicurso de introdução ao Machine Learning oferecido pelo [PET - Física](https://petfisica.home.blog)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981d88c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "O conteúdo é mantido no [GitHub]() e distribuídos sob uma [licença BSD3](https://opensource.org/licenses/BSD-3-Clause)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540a7c9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Veja a tabela de conteúdos]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef0183",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Este `Notebook` pode, opcionalmente, ser visto como uma [apresentação de slides](https://medium.com/learning-machine-learning/present-your-data-science-projects-with-jupyter-slides-75f20735eb0f). Clique [aqui]() para ver os slides online ou, para apresentar os slides localmente, use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94c6e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```Python\n",
    "$ jupyter nbconvert Dia2.ipynb --to slides --post serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbe86d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6f6fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98302e88",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "def opt_plot():\n",
    "    # plt.style.use('dark_background')\n",
    "    plt.grid(True, linestyle=':', color='0.50')\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(axis='both',which='minor', direction = \"in\",\n",
    "                        top = True,right = True, length = 5,width = 1,labelsize = 15)\n",
    "    plt.tick_params(axis='both',which='major', direction = \"in\",\n",
    "                        top = True,right = True, length = 8,width = 1,labelsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ee725",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Versões das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03aabab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.9.8 64bit [MSC v.1929 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "8.0.1"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.19043"
        },
        {
         "module": "Matplotlib",
         "version": "3.5.1"
        },
        {
         "module": "Numpy",
         "version": "1.22.2"
        },
        {
         "module": "Pandas",
         "version": "1.4.0"
        },
        {
         "module": "Seaborn",
         "version": "0.11.2"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.9.8 64bit [MSC v.1929 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>8.0.1</td></tr><tr><td>OS</td><td>Windows 10 10.0.19043</td></tr><tr><td>Matplotlib</td><td>3.5.1</td></tr><tr><td>Numpy</td><td>1.22.2</td></tr><tr><td>Pandas</td><td>1.4.0</td></tr><tr><td>Seaborn</td><td>0.11.2</td></tr><tr><td colspan='2'>Sat Jun 04 19:16:25 2022 Hora Padrão de Buenos Aires</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.9.8 64bit [MSC v.1929 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 8.0.1 \\\\ \\hline\n",
       "OS & Windows 10 10.0.19043 \\\\ \\hline\n",
       "Matplotlib & 3.5.1 \\\\ \\hline\n",
       "Numpy & 1.22.2 \\\\ \\hline\n",
       "Pandas & 1.4.0 \\\\ \\hline\n",
       "Seaborn & 0.11.2 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Sat Jun 04 19:16:25 2022 Hora Padrão de Buenos Aires} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.9.8 64bit [MSC v.1929 64 bit (AMD64)]\n",
       "IPython 8.0.1\n",
       "OS Windows 10 10.0.19043\n",
       "Matplotlib 3.5.1\n",
       "Numpy 1.22.2\n",
       "Pandas 1.4.0\n",
       "Seaborn 0.11.2\n",
       "Sat Jun 04 19:16:25 2022 Hora Padrão de Buenos Aires"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information Matplotlib, Numpy, Pandas, Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a29a0c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Instalação:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba40d6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```\n",
    "$ pip install version_information\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5967050",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73c03c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Carregamos apenas os módulos `scikit-learn` que precisamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ceed8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0dba2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baixando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbbece",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43525e2a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313886e9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cc56ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptando métodos lineares para resolver problemas não lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6ed72",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0b23b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. A Cura da Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9209e3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Já encontramos a \"maldição da dimensionalidade\" no contexto da redução da dimensionalidade, mas às vezes uma grande dimensionalidade pode ser uma cura. Como exemplo motivador, considere os dados 2D plotados abaixo, que claramente contêm dois clusters com formas altamente não lineares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e00d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efff8f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d8ab69",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "O gráfico acima é colorido usando os rótulos verdadeiros armazenados na coluna `y` de `circles_targets`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f6fef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos usar o KMeans para ajustar esses dados como dois *clusters* e plote os resultados usando `plot_circles(labels=fit.labels_)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87930be",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33305b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Os *clusters* encontrados pelo KMeans não são o que queremos, mas também não surpreende, dado que o KMeans particiona as amostras com uma linha divisória simples (ou hiperplano em dimensões superiores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e702d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos criar um novo conjunto de dados chamado `circles_3d` que é uma cópia de `circles_data`, mas com um novo recurso adicionado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6e7d5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$x_{2}=x_{0}^{2}+x_{1}^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7be92b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pense em como esse novo recurso altera o problema de *clustering*, se for o caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c46589",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b9e1125",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ao *aumentar* a dimensionalidade de nossos dados, transformamos um problema de agrupamento muito não linear em um problema linear trivial! Para ver isso, plote os dados em 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa45e6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8460e6cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finalmente, ajustamos para 2 *clusters* KMeans em nossos novos dados `circles_3d` e plotamos os resultados, como acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4753657",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f564bac7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Este é exatamente o resultado que queríamos, mas não tão surpreendente depois de ver o gráfico 3D acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d2dd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Funções Kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd4bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Existem muitas classes de problemas em que não linearidades em seus dados podem ser tratadas com métodos lineares, primeiro embutindo em uma dimensão mais alta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d00bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A incorporação que usamos na seção anterior foi escolhida a dedo para esses dados, mas uma incorporação genérica geralmente funcionará se adicionar dimensões suficientes. Por exemplo, a função abaixo é comumente usada para incorporar recursos 2D $(x_{0},x_{1})$ em um espaço 7D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0a3aa",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\phi(x_0, x_1) = \\begin{pmatrix}\n",
    "x_0^2 \\\\\n",
    "x_0 x_1 \\\\\n",
    "x_1 x_0 \\\\\n",
    "x_1^2 \\\\\n",
    "\\sqrt{2 c} x_0 \\\\\n",
    "\\sqrt{2 c} x_1 \\\\\n",
    "c\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73144e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5c9ebd7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Um gráfico de pares dos `circles_data` incorporados em 7D mostra que essa é uma incorporação peculiar, mas permite uma separação linear dos dois clusters (por meio de seus componentes $x^{2}_{0}$ e $x^{2}_{1}$). Também parece ineficiente, com um recurso repetido $(x_{0}x_{1})$ e outro constante $(c)$. No entanto, este é apenas o membro mais simples de uma família de embeddings onde c desempenha um papel importante na fixação da normalização relativa dos diferentes grupos de [monômios](https://en.wikipedia.org/wiki/Monomial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0bd76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be2cbbe4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A razão para escolher esta incorporação peculiar é que ela tem a seguinte propriedade muito útil:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57bb122",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\phi(X_{i})\\cdot\\phi(X_{j})=(X_{i}\\cdot X_{j}+c)^{2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1172fd3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "onde $X_{i}$ e $X_{j}$ são amostras (*features*) arbitrárias (linhas) de nossos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff042b89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primeiro, vamos verificar isso explicitamente para dados de círculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed019d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12fd25e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A razão pela qual essa propriedade é tão útil é que o RHS pode ser avaliado muito mais rápido do que o LHS e nunca exige que realmente incorporemos nossas amostras originais no espaço de dimensão superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594298e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As funções no espaço amostral que avaliam um produto escalar em um espaço diferente são chamadas de **funções kernel**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd677fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$K(X_{i},X_{j})=\\phi(X_{i})\\cdot\\phi(X_{j}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf054a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Uma função kernel é uma [medida de similaridade](https://en.wikipedia.org/wiki/Similarity_measure), pois mede a similaridade das amostras $i$ e $j$, com valor máximo para amostras idênticas e zero para amostras ortogonais. As medidas de similaridade estão relacionadas a medidas de distância (por exemplo, métricas na relatividade), mas com o comportamento oposto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffdcb8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- amostras muito semelhantes: distância ~ 0, grande semelhança.\n",
    "- amostras muito diferentes: grande distância, similaridade ~ 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d5cd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A importância das funções do kernel é mais profunda do que apenas sua eficiência computacional: muitos algoritmos podem ser expressos usando apenas produtos de ponto entre amostras e, portanto, podem ser aplicados a dados incorporados em uma dimensão superior sem nunca fazer a incorporação. Esse insight é conhecido como o **truque do kernel**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c4747",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Escolha uma função de kernel $K$.\n",
    "- Escolha um algoritmo que possa ser expresso usando apenas produtos escalares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f8847",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Quando esses pré-requisitos são atendidos, o algoritmo pode ser aplicado de maneira fácil e eficiente a dados que são efetivamente impulsionados para um espaço de alta dimensão. Como vimos no exemplo acima, o principal benefício é que os dados não lineares agora podem ser analisados usando métodos lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6dba7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Infelizmente, há um número limitado de funções de kernel adequadas $K$ (comece com o [teorema de Mercer](https://en.wikipedia.org/wiki/Mercer%27s_theorem) se você estiver interessado em aprender mais sobre o porquê disso). Já conhecemos o [kernel polinomial](https://en.wikipedia.org/wiki/Polynomial_kernel), que pode ser escrito de forma mais geral como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d2a00",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$K(X_{i},X_{j})=(\\gamma X_{i}\\cdot X_{j}+c)^{d},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801798c1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "onde $\\gamma$, $c$ e $d$ são todos hiperparâmetros (nosso exemplo anterior usou $\\gamma=1$, $c=1$ e $d=2$). O módulo `metric.pairwise` do `sklearn` pode calcular a matriz de todos os produtos escalares de amostra possíveis para este e outros kernels, por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b9f89",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee60a5ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Outros kernels populares são o [kernel sigmoid](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.sigmoid_kernel.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8cfe5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\boxed{K(X_{i},X_{j})=\\tanh{(\\gamma X_{i}\\cdot X_{j}+c)}},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc064108",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Além disso, o [kernel da função de base radial (rbf)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) (cujo embutimento é de dimensão infinita devido à expansão em série infinita de $e^{−x}$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9407b04",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\boxed{K(X_{i},X_{j})=\\exp{\\Bigg(-\\gamma|X_{i}-X_{j}|^{2}\\Bigg)}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24a45c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156964e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Como um exemplo do **truque do kernel**, o algoritmo PCA pode ser adaptado para usar apenas produtos de ponto para projetar cada amostra nos autovetores de dimensão superior. O [algoritmo KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) resultante é usado exatamente como os métodos de decomposição linear, mas com alguns hiperparâmetros adicionais, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf13771",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```Python\n",
    "fit = decomposition.KernelPCA(n_components=d, kernel='poly', gamma=1., degree=2).fit(X)\n",
    "Y = fit.transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc30ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Uma limitação do truque do kernel para PCA é que as amostras originais não podem ser reconstruídas usando apenas produtos de ponto, de modo que a reconstrução das variáveis latentes se torna um novo problema desafiador que requer uma abordagem de [aprendizado de máquina supervisionada separada](http://papers.nips.cc/paper/2417-learning-to-find-pre-images.pdf). Felizmente, a implementação do `sklearn` cuida de tudo isso para você:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4dafb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```Python\n",
    "fit = decomposition.KernelPCA(n_components=d, kernel='poly', gamma=1., degree=2, inverse_transform=True).fit(X)\n",
    "Y = fit.transform(X)\n",
    "reconstructed = fit.inverse_transform(Y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f290d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos a seguinte função para demonstrar o método KernelPCA, que permite definir o hiperparâmetro $\\gamma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c02166",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "798118dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O resultado é espetacular: nossos dados não lineares são completamente linearizados quando transformados em um espaço variável latente 2D. Observe que, neste exemplo, não estamos realizando nenhuma redução geral de dimensionalidade: começamos com 2 recursos, expandimos implicitamente para um número infinito de recursos usando o kernel RBF, depois reduzimos para duas variáveis latentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dd2d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51dd68d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Os resultados acima são bastante sensíveis à escolha dos hiperparâmetros. Para explorar isso, vamos executar novamente `kpca_demo` com diferentes valores de $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6af6d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ff857f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Os resultados são bem ajustados e pequenas variações em $\\gamma$ podem destruir a separação linear. A sensibilidade a $\\gamma$ não é muito surpreendente, pois é um parâmetro da função kernel. No entanto, os resultados do KernelPCA também podem mudar drasticamente com uma pequena alteração nos dados de entrada. Consulte este [problema do github](https://github.com/scikit-learn/scikit-learn/issues/10530) para obter detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a691ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Incorporação linear local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7811c6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "O **truque do kernel** não é a única maneira de aproveitar métodos lineares para problemas não lineares. Para o nosso próximo exemplo, consideramos a **incorporação linear local** (LLE), que é um tipo de \"aprendizagem múltipla\", ou seja, um método de redução de dimensionalidade para dados em uma variedade não linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80aa3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primeiro vamos ver alguns dados 3D que são claramente 2D, mas requerem uma decomposição não linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9bdef",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7617076e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O pairplot é confuso até que você veja o seguinte gráfico onde, além disso, cada ponto é colorido de acordo com sua coordenada 1D verdadeira ao longo da direção principal do coletor (armazenado na coluna `y` de `ess_targets`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4a212",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb4da4a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O LLE aproveita o fato de que a variedade é \"plana\" na vizinhança de cada amostra, portanto pode ser descrita localmente com uma aproximação linear. Construímos uma aproximação linear local para uma amostra $\\overrightarrow{X_{i}}$ como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7ccd2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\boxed{\\overrightarrow{X_{i}}\\approxeq\\sum_{j\\neq i}W_{ij}\\overrightarrow{X_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72c402",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "em relação aos pesos $n\\times N$ em $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cf256",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O principal insight do LLE é que, uma vez que um conjunto adequado de pesos $W$ tenha sido encontrado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0f736",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- eles descrevem completamente a geometria local do coletor, e\n",
    "- esta geometria pode então ser *transferida* para outro espaço (menor) de variáveis latentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69c2ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A maneira como isso funciona é minimizar uma segunda função objetivo muito semelhante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e7c8a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\\boxed{\\sum_{i}\\Bigg|\\overrightarrow{Y_{i}}-\\sum_{j\\neq i}W_{ij}\\overrightarrow{X_{j}}\\Bigg|^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55fa48",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "onde cada amostra $X_{i}$ tem um $Y_{i}$ correspondente, mas estes podem ter dimensões completamente diferentes! Observe que, embora as funções de metas pareçam semelhantes, os parâmetros que minimizamos são diferentes a cada vez:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856c231",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Primeiro minimizamos em relação aos elementos de $W$, com os dados de entrada $X$ fixos.\n",
    "\n",
    "- Em seguida, minimizamos em relação às variáveis latentes $Y_{i}$, com a matriz de pesos $W$ fixa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442c59d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Este método foi descoberto em 2000 e o [artigo original](https://www.science.org/doi/10.1126/science.290.5500.2323) é bastante acessível."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c5e59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O [método LLE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) reside no módulo `sklearn` chamado `manifold` e segue o padrão de chamada usual, com dois hiperparâmetros significativos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb603562",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- O número de vizinhos mais próximos a serem usados para calcular $W$.\n",
    "\n",
    "- O número de variáveis latentes (componentes) a serem usadas em $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd2b38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para obter resultados reproduzíveis, você também deve passar um RandomState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f63e29",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7537ed0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Depois de projetar no espaço latente, descobrimos que a forma em S foi efetivamente achatada, embora não em um belo retângulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc21b37",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8cc65df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparamos isso com o que um PCA linear encontra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3098a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "858ff8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ou um KernelPCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bc469",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7576796",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A classe `sklearn` LLE também fornece algumas variantes de LLE que podem ter um desempenho ainda melhor nesse problema, por exemplo (observe os `n_neighbors` maiores necessários - outro exemplo de ajuste fino):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e39155",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ebf7117",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](https://c.tenor.com/hEOM8E4epvgAAAAC/hahaha-thats-all-folks.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb90c3a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
